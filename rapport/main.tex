\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

% Title
\title{Implementing RaptorQ in Hadoop\\
\large ICT3 Project n°1 — Shanghai Jiao Tong University}
\author{Alexandra Baron \and Mathis Liens \and Charles Pelong\and Baptiste Halçaren \and Maria Stivala \and Arnaud Brisset}
\date{October 2025}

\begin{document}
\maketitle

% University logo - Placed AFTER title and smaller
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\linewidth]{images/shanghai-jiao-tong-university.png}
\end{figure}

\begin{abstract}
This report presents the implementation of RaptorQ erasure coding as an alternative to Reed-Solomon in Hadoop 3.4.2. We leverage the OpenRQ library, an open-source RFC 6330 RaptorQ implementation, and integrate it into Hadoop's three-layer erasure coding architecture. Our implementation adapts RaptorQ's fountain code model to Hadoop's fixed-parity requirements through deterministic ESI mapping and parity regeneration strategies. Comprehensive unit tests validate encoding and decoding correctness across various erasure scenarios. This work demonstrates the extensibility of Hadoop's erasure coding framework while providing insights into the trade-offs between deterministic and probabilistic erasure coding approaches.
\end{abstract}

\section{Introduction}

Distributed storage systems face the challenge of maintaining data durability and availability in the presence of node failures. Traditional replication strategies, while simple, incur significant storage overhead. Erasure coding (EC) provides a storage-efficient alternative by encoding data into $k$ data blocks and $m$ parity blocks, allowing reconstruction of the original data from any $k$ out of $k+m$ blocks.

Hadoop 3.x introduced erasure coding support, with Reed-Solomon (RS) as the primary algorithm. While RS is deterministic and well-understood, it has limitations in certain scenarios, particularly in streaming applications where fountain codes like RaptorQ offer advantages.

This project aims to implement RaptorQ erasure coding in Hadoop 3.4.2, providing an alternative to Reed-Solomon that offers probabilistic decoding and the ability to generate an unlimited number of repair symbols. The implementation integrates the OpenRQ library, a Java implementation of RFC 6330 RaptorQ, into Hadoop's existing erasure coding framework.

\section{Background and Motivation}

\subsection{Erasure Coding in Hadoop}

Hadoop implements a three-layer erasure coding architecture that separates concerns and enables extensibility. Table \ref{tab:comparison} provides a comparison between Reed-Solomon and RaptorQ approaches.

\begin{table}[h]
\centering
\begin{tabular}{|p{5cm}|p{5cm}|}
\hline
\textbf{Reed-Solomon} & \textbf{RaptorQ} \\\hline
Deterministic (exact $k$ symbols) & Fountain code ($k+\epsilon$ symbols) \\\hline
Finite field arithmetic (GF256) & LT code + pre-coding \\\hline
Fixed $m$ parity symbols & Unlimited repair symbols \\\hline
Well-optimized native implementations & Moderate performance (Java) \\\hline
Always succeeds with $k$ symbols & Probabilistic (rare failures) \\\hline
\end{tabular}
\caption{\label{tab:comparison}Key differences between Reed-Solomon and RaptorQ}
\end{table}

Hadoop's current architecture expects exactly $m$ parity blocks, requiring adaptation of RaptorQ's unlimited symbol generation model while preserving its fountain code properties.

\section{Hadoop Erasure Coding Architecture}

Hadoop's erasure coding follows a three-layer architecture (illustrated in Figure \ref{fig:architecture}):

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{diagrams/hadoop_architecture.png}
\caption{\label{fig:architecture}Hadoop erasure coding architecture with RaptorQ integration}
\end{figure}

\begin{itemize}
\item \textbf{Codec Layer}: Defines codec interfaces and creates encoders/decoders. We implemented \texttt{RaptorQErasureCodec} following the pattern of existing codecs.
\item \textbf{Coder Layer}: Manages block groups and delegates to raw coders. \texttt{RaptorQEncoder/Decoder} handle Hadoop-specific concerns.
\item \textbf{RawCoder Layer}: Performs mathematical operations on byte arrays/ByteBuffers. Our \texttt{RaptorQRawEncoder/Decoder} integrate with OpenRQ.
\end{itemize}

ServiceLoader registration enables automatic factory discovery. Configuration maps codec names to classes via \texttt{io.erasurecode.codec.raptorq}.

\section{RaptorQ and OpenRQ Library}

RaptorQ (RFC 6330) is a fountain code that encodes data into fixed-size symbols and can generate unlimited repair symbols. Any $k$ independent symbols suffice for reconstruction with high probability.

OpenRQ provides the Java implementation with key classes: \texttt{FECParameters} (encoding parameters), \texttt{DataEncoder/Decoder} (high-level interfaces), and \texttt{SourceBlockEncoder/Decoder} (per-block operations).

\subsection{Adaptation Challenges}

The primary challenge is adapting RaptorQ's unlimited symbol model to Hadoop's fixed-$m$ requirement. Figure \ref{fig:esi-mapping} illustrates our deterministic ESI mapping strategy.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{diagrams/esi_mapping.png}
\caption{\label{fig:esi-mapping}ESI mapping: Hadoop blocks to RaptorQ symbols}
\end{figure}

We map data blocks to source symbols (ESI $0..k-1$) and parity blocks to repair symbols (ESI $k..k+m-1$), ensuring deterministic parity generation while preserving fountain code properties.

\section{Implementation}

We implement across Hadoop's three layers: \texttt{RaptorQErasureCodec} (codec), \texttt{RaptorQEncoder/Decoder} (coders), and \texttt{RaptorQRawEncoder/Decoder} (raw coders with OpenRQ).

Figures \ref{fig:encoding} and \ref{fig:decoding} illustrate the encoding and decoding flows.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{diagrams/raptorq_encoding.png}
\caption{\label{fig:encoding}RaptorQ encoding flow in Hadoop}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{diagrams/raptorq_decoding.png}
\caption{\label{fig:decoding}RaptorQ decoding flow with parity regeneration}
\end{figure}

\textbf{Encoding}: Concatenate $k$ chunks ($k \times T$ bytes), create \texttt{FECParameters}, generate $m$ repair symbols (ESI $k..k+m-1$), write to parity outputs.

\textbf{Decoding}: Build matching \texttt{FECParameters}, feed available symbols to decoder, extract recovered data; for erased parity, re-encode to regenerate.

\subsection{Implementation Details}

We support both \texttt{byte[]} and \texttt{ByteBuffer} APIs for performance. ServiceLoader registration makes the factory discoverable as \texttt{raptorq\_java}. We added a built-in \texttt{RAPTORQ\_6\_3} policy ($k=6, m=3$) to HDFS, enabling administrative selection alongside RS and XOR.

Error handling: failures from insufficient symbols throw \texttt{IOException} (rare in practice). Future work could add configurable overhead margins.

\section{Testing and Validation}

Our test strategy (Figure \ref{fig:testing}) covers encoding/decoding correctness, multiple erasures, dual API paths, and randomized data.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{diagrams/test_scenarios.png}
\caption{\label{fig:testing}Test scenarios and validation strategy}
\end{figure}

Scenarios: $(k=6, m=3)$ with 2–3 erasures (byte[] and ByteBuffer), verifying data recovery, parity regeneration, and mixed erasures. All tests pass with byte-level equality validation.

\section{Challenges and Design Decisions}

\subsection{Key Challenges}

\begin{enumerate}
\item \textbf{Fixed vs. Flexible Parity}: Select first $m$ repair symbols (ESI $k..k+m-1$) for determinism.
\item \textbf{Decoding Model}: Reconstruct all source data, then extract requested symbols and regenerate parity if needed.
\item \textbf{Memory Usage}: Accept $k \times T$ buffer concatenation; streaming optimization deferred.
\item \textbf{Determinism}: Fixed ESI mapping ensures consistent outputs while preserving fountain properties.
\item \textbf{Error Handling}: Throw \texttt{IOException} on rare failures; consider overhead margins later.
\end{enumerate}

\section{Conclusion and Future Work}

\subsection{Summary}

We successfully integrated RaptorQ into Hadoop 3.4.2, following the three-layer architecture. The implementation adds a deterministic parity strategy using OpenRQ, includes unit tests, and registers an HDFS policy for administrative use.

\subsection{Limitations}

We assume equal chunk sizes, concatenate inputs into a single buffer (memory-intensive for large $k/T$), use baseline OpenRQ performance without native optimizations, and handle failures via exceptions without automatic recovery.

\subsection{Future Enhancements}

Potential improvements: streaming for large configurations, padding for variable-sized chunks, native SIMD optimizations, configurable overhead margins, automatic retries, Reed-Solomon benchmarks, and production stress testing.

\subsection{Reflections}

This project provided valuable insights into:
\begin{itemize}
\item The design and extensibility of large-scale distributed systems
\item The trade-offs between different erasure coding algorithms
\item The challenges of adapting theoretical algorithms to practical system constraints
\item The importance of maintaining compatibility while introducing new features
\end{itemize}

The implementation demonstrates that Hadoop's erasure coding architecture is well-designed for extensibility, enabling the integration of new algorithms with minimal modifications to existing code. The layered approach successfully separates concerns, making the codebase maintainable and testable.
%\bibliographystyle{alpha}
%\bibliography{references}

\end{document}
